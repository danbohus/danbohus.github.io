<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Frameset//EN"
   "http://www.w3.org/TR/html4/frameset.dtd">
<HTML>
<head>
    <title>Dan Bohus [Publications by year]</title>
    <link rel="STYLESHEET" href="mystyles.css" type="text/css" />    
</head>
<body style="margin:0px;height:100%">
    <script type="text/javascript">
        var _height = null;
        function sh(_d) {
            var _x = document.getElementById(_d);
            if (_height == null) _height = _x.style.height;
            _x.style.display = _x.style.display == "block" ? "none" : "block";
        }
    </script>
<div>
    <div style="background:#DDDDDD;width:100%;position:fixed;top:0px;z-index:2">
    <table border="0" cellspacing="0" cellpadding="0" width="800px" align="center">
        <tbody>
            <tr class="navigation_header_top">
                <td>
                </td>
                <td>
                </td>
                <td>
                </td>
                <td>
                </td>
            </tr>
            <tr class="navigation_header_bottom">
                <td>
                    <a class="navigation" href="index.html">&nbsp;&nbsp;Home&nbsp;&nbsp;</a>
                </td>
                <!--<td>
                    &nbsp;&nbsp;Research&nbsp;&nbsp;
                </td>-->
                <td style="background:#ffffff">
                    <span style="color:#000000">&nbsp;&nbsp;Publications&nbsp;&nbsp;</span>
                </td>
                <td width="100%">
                </td>
            </tr>
        </tbody>
    </table>
    </div>
    <div style="background:#FFFFFF;width:100%;position:fixed;top:75px;height:10px;z-index:2">
    </div>
    <div style="width:100%;position:absolute;top:100px;z-index:1">
    <table width="800px" border="0" cellspacing="0" cellpadding="0" align="center">
        <tr class="section_header">
            <td class="section_header" width="800px">
                2014
            </td>
        </tr>
        <tr class="separator">
            <td>
            </td>
        </tr>
        <tr>
            <td width="800px">
                <table border="0" cellspacing="0" cellpadding="0">
                    <tr>
                        <td valign="top" width="10" class="paper">
                            -
                        </td>
                        <td class="paper">
                            <b>Bohus, D.</b>, Horvitz, E., (2014) - <a class="normal" href="http://dl.acm.org/authorize.cfm?key=N82877"><i>Managing Human-Robot Engagement with Forecasts and ... um ... Hesitations</i></a>, in Proceedings of ICMI'2014,
                            Istanbul, Turkey [<a class="normal" target="_self" href="javascript:sh('icmi_2014a')">abs</a>] 
                        </td>
                    </tr>
                    <tr>
                        <td>
                        </td>
                        <td>
                            <div id="icmi_2014a" style="display: none">
                                <table class="normal" width="100%">
                                    <tr>
                                        <td valign="top" width="20">
                                            <b>&nbsp;</b>
                                        </td>
                                        <td class="paper_abstract">
We explore methods for managing conversational engagement in
open-world, physically situated dialog systems. We investigate a
self-supervised methodology for constructing forecasting models
that aim to anticipate when participants are about to terminate their
interactions with a situated system. We study how these models can
be leveraged to guide a disengagement policy that uses linguistic
hesitation actions, such as filled and non-filled pauses, when
uncertainty about the continuation of engagement arises. The
hesitations allow for additional time for sensing and inference, and
convey the system’s uncertainty. We report results from a study of
the propose</td>
                                    </tr>
                                </table>
                            </div>
                        </td>
                    </tr>
                </table>
            </td>
        </tr>
        <tr class="separator">
            <td>
            </td>
        </tr>
        <tr>
            <td width="800px">
                <table border="0" cellspacing="0" cellpadding="0">
                    <tr>
                        <td valign="top" width="10" class="paper">
                            -
                        </td>
                        <td class="paper">
                            Pejsa, T., <b>Bohus, D.</b>, Cohen, M., Saw, C.W., Mahoney, J., Horvitz, E. (2014) - 
                                    <a class="normal" href="http://dl.acm.org/authorize.cfm?key=N82813"><i>Natural Communication about Uncertainties in Situated Interaction</i></a>, in ICMI'2014,
                            Istanbul, Turkey [<a class="normal" target="_self" href="javascript:sh('icmi_2014b')">abs</a>] [<a class="normal" target="_self" href="videos/ICMI2014Demo.mp4">supplemental video</a>]
                        </td>
                    </tr>
                    <tr>
                        <td>
                        </td>
                        <td>
                            <div id="icmi_2014b" style="display: none">
                                <table class="normal" width="100%">
                                    <tr>
                                        <td valign="top" width="20">
                                            <b>&nbsp;</b>
                                        </td>
                                        <td class="paper_abstract">
Physically situated, multimodal interactive systems must often grapple with uncertainties about properties of the world, people, and their intentions and actions. We present methods for estimating and communicating about different uncertainties in situated interaction, leveraging the affordances of an embodied conversational agent. The approach harnesses a representation that captures both the magnitude and the sources of uncertainty, and a set of policies that select and coordinate the production of nonverbal and verbal behaviors to communicate the system’s uncertainties to conversational participants. The methods are designed to enlist participants’ help in a natural manner to resolve uncertainties arising during interactions. We report on a preliminary implementation of the proposed methods in a deployed system and illustrate the functionality with a trace from a sample interaction.</td>
                                    </tr>
                                </table>
                            </div>
                        </td>
                    </tr>
                </table>
            </td>
        </tr>
        <tr class="separator">
            <td>
            </td>
        </tr>
        <tr>
            <td width="800px">
                <table border="0" cellspacing="0" cellpadding="0">
                    <tr>
                        <td valign="top" width="10" class="paper">
                            -
                        </td>
                        <td class="paper">
                            Mitchell, M., <b>Bohus, D.</b>, Kamar, E., (2014) - <a class="normal" href="http://dl.acm.org/citation.cfm?id=2540128.2540497">
                                    <i>Crowdsourcing Language Generation Templates for Dialogue Systems</i></a>, in INLG'2014,
                            Philadelphia, PA, USA [<a class="normal" target="_self" href="javascript:sh('inlg_2014')">abs</a>]
                        </td>
                    </tr>
                    <tr>
                        <td>
                        </td>
                        <td>
                            <div id="inlg_2014" style="display: none">
                                <table class="normal" width="100%">
                                    <tr>
                                        <td valign="top" width="20">
                                            <b>&nbsp;</b>
                                        </td>
                                        <td class="paper_abstract">
We explore the use of crowdsourcing to
generate natural language in spoken dialogue
systems. We introduce a methodology
to elicit novel templates from the
crowd based on a dialogue seed corpus,
and investigate the effect that the amount
of surrounding dialogue context has on the
generation task. Evaluation is performed
both with a crowd and with a system developer
to assess the naturalness and suitability
of the elicited phrases. Results indicate
that the crowd is able to provide reasonable
and diverse templates within this
methodology. More work is necessary before
elicited templates can be automatically
plugged into the system.                                        </td>
                                    </tr>
                                </table>
                            </div>
                        </td>
                    </tr>
                </table>
            </td>
        </tr>
        <tr class="separator">
            <td>
            </td>
        </tr>
        <tr>
            <td width="800px">
                <table border="0" cellspacing="0" cellpadding="0">
                    <tr>
                        <td valign="top" width="10" class="paper">
                            -
                        </td>
                        <td class="paper">
                            <b>Bohus, D.</b>, Saw, C.W., Horvitz, E., (2014) - <a class="normal" href="http://dl.acm.org/citation.cfm?id=2615835">
                                    <i>Directions Robot: In-the-Wild Experiences and Lessons Learned</i></a>, in AAMAS'2014,
                            Paris, France [<a class="normal" target="_self" href="javascript:sh('aamas_2014')">abs</a>]
                        </td>
                    </tr>
                    <tr>
                        <td>
                        </td>
                        <td>
                            <div id="aamas_2014" style="display: none">
                                <table class="normal" width="100%">
                                    <tr>
                                        <td valign="top" width="20">
                                            <b>&nbsp;</b>
                                        </td>
                                        <td class="paper_abstract">
We introduce Directions Robot, a system we have fielded for
studying open-world human-robot interaction. The system brings
together models for situated spoken language interaction with
directions-generation and a gesturing humanoid robot. We describe
the perceptual, interaction, and output generation competencies of
this system. We then discuss experiences and lessons drawn from
data collected in an initial in-the-wild deployment, and highlight
several challenges with managing engagement, providing
directions, and handling out-of-domain queries that arise in openworld,
multiparty settings.                                        </td>
                                    </tr>
                                </table>
                            </div>
                        </td>
                    </tr>
                </table>
            </td>
        </tr>
        <tr class="large_separator">
            <td>
            </td>
        </tr>
        <tr class="section_header">
            <td class="section_header" width="800px">
                2013
            </td>
        </tr>
        <tr class="separator">
            <td>
            </td>
        </tr>
        <tr>
            <td width="800px">
                <table border="0" cellspacing="0" cellpadding="0">
                    <tr>
                        <td valign="top" width="10" class="paper">
                            -
                        </td>
                        <td class="paper">
                            Rosenthal, S., <b>Bohus, D.</b>, Kamar, E., Horvitz, E., (2013) - <a class="normal" href="http://dl.acm.org/citation.cfm?id=2540128.2540497">
                                    <i>Look versus Leap: Computing Value of Information with High-Dimensional Streaming Evidence</i></a>, in ACL'2013,
                            Beijing, China [<a class="normal" target="_self" href="javascript:sh('ijcai_2013')">abs</a>]
                        </td>
                    </tr>
                    <tr>
                        <td>
                        </td>
                        <td>
                            <div id="ijcai_2013" style="display: none">
                                <table class="normal" width="100%">
                                    <tr>
                                        <td valign="top" width="20">
                                            <b>&nbsp;</b>
                                        </td>
                                        <td class="paper_abstract">
                                         A key decision facing autonomous systems with access
to streams of sensory data is whether to act
based on current evidence or to wait for additional
information that might enhance the utility of taking
an action. Computing the value of information
is particularly difficult with streaming highdimensional
sensory evidence. We describe a belief
projection approach to reasoning about information
value in these settings, using models for inferring
future beliefs over states given streaming evidence.
These belief projection models can be learned from
data or constructed via direct assessment of parameters
and they fit naturally in modular, hierarchical
state inference architectures. We describe principles
of using belief projection and present results
drawn from an implementation of the methodology
within a conversational system.
                                        </td>
                                    </tr>
                                </table>
                            </div>
                        </td>
                    </tr>
                </table>
            </td>
        </tr>
        <tr class="separator">
            <td>
            </td>
        </tr>
        <tr>
            <td width="800px">
                <table border="0" cellspacing="0" cellpadding="0">
                    <tr>
                        <td valign="top" width="10" class="paper">
                            -
                        </td>
                        <td class="paper">
                            Rosenthal, S., Skaff, S., Veloso, M., <b>Bohus, D.</b>, Horvitz, E., (2013) - <a class="normal" href="http://dl.acm.org/citation.cfm?id=2447643">
                                    <i>Execution Memory for Grounding and Coordination</i></a>, in HRI'2013,
                            Tokyo, Japan [<a class="normal" target="_self" href="javascript:sh('hri_2013')">abs</a>]
                        </td>
                    </tr>
                    <tr>
                        <td>
                        </td>
                        <td>
                            <div id="hri_2013" style="display: none">
                                <table class="normal" width="100%">
                                    <tr>
                                        <td valign="top" width="20">
                                            <b>&nbsp;</b>
                                        </td>
                                        <td class="paper_abstract">
As robots are introduced into human environments
for long periods of time, human owners and collaborators will
expect them to remember shared events that occur during execution.
Beyond naturalness of having memories about recent and
longer-term engagements with people, such execution memories
can be important in tasks that persist over time by allowing
robots to ground their dialog and to refer efficiently to previous
events. In this work, we define execution memory as the capability
of saving interaction event information and recalling it for later
use. We divide the problem into four parts: salience filtering of
sensor evidence and saving to short term memory, archiving from
short to long term memory and caching from long to short term
memory, and recalling memories for use in state inference and
policy execution. We then provide examples of how execution
memory can be used to enhance user experience with robots.
                                        </td>
                                    </tr>
                                </table>
                            </div>
                        </td>
                    </tr>
                </table>
            </td>
        </tr>
        <tr class="separator">
            <td>
            </td>
        </tr>
        <tr>
            <td width="800px">
                <table border="0" cellspacing="0" cellpadding="0">
                    <tr>
                        <td valign="top" width="10" class="paper">
                            -
                        </td>
                        <td class="paper">
                            Metallinou, A., <b>Bohus, D.</b>, Williams, J.D., (2013) - <a class="normal" href="http://aclanthology.info/papers/discriminative-state-tracking-for-spoken-dialog-systems">
                                    <i>Discriminative state tracking for spoken dialog systems</i></a>, in ACL'2013,
                            Sofia, Bulgaria [<a class="normal" target="_self" href="javascript:sh('acl_2013')">abs</a>]
                        </td>
                    </tr>
                    <tr>
                        <td>
                        </td>
                        <td>
                            <div id="acl_2013" style="display: none">
                                <table class="normal" width="100%">
                                    <tr>
                                        <td valign="top" width="20">
                                            <b>&nbsp;</b>
                                        </td>
                                        <td class="paper_abstract">
                                            In spoken dialog systems, statistical state
tracking aims to improve robustness to speech
recognition errors by tracking a posterior distribution
over hidden dialog states. Current
approaches based on generative or discriminative
models have different but important shortcomings
that limit their accuracy. In this paper
we discuss these limitations and introduce
a new approach for discriminative state tracking
that overcomes them by leveraging the
problem structure. An offline evaluation with
dialog data collected from real users shows
improvements in both state tracking accuracy
and the quality of the posterior probabilities.
Features that encode speech recognition error
patterns are particularly helpful, and training
requires relatively few dialogs.
                                        </td>
                                    </tr>
                                </table>
                            </div>
                        </td>
                    </tr>
                </table>
            </td>
        </tr>
        <tr class="separator">
            <td>
            </td>
        </tr>
        <tr>
            <td width="800px">
                <table border="0" cellspacing="0" cellpadding="0">
                    <tr>
                        <td valign="top" width="10" class="paper">
                            -
                        </td>
                        <td class="paper">
                            Lasecki, W.S., Kamar, E., <b>Bohus, D.</b> (2013) - <a class="normal" href="http://www.aaai.org/ocs/index.php/HCOMP/HCOMP13/paper/view/7637/7477">
                                    <i>Conversations in the Crowd: Collecting Data for Task-Oriented Dialog Learning</i></a>, in HCOMP'2013,
                            Palm Springs, CA, USA [<a class="normal" target="_self" href="javascript:sh('hcomp_2013')">abs</a>]
                        </td>
                    </tr>
                    <tr>
                        <td>
                        </td>
                        <td>
                            <div id="hcomp_2013" style="display: none">
                                <table class="normal" width="100%">
                                    <tr>
                                        <td valign="top" width="20">
                                            <b>&nbsp;</b>
                                        </td>
                                        <td class="paper_abstract">
                                            A major challenge in developing dialog systems
is obtaining realistic data to train the systems
for specic domains. We study the opportunity
for using crowdsourcing methods to collect dialog
datasets. Specically, we introduce ChatCollect, a
system that allows researchers to collect conversations focused around denable tasks from pairs of
workers in the crowd. We demonstrate that varied and in-depth dialogs can be collected using
this system, then discuss ongoing work on creating a crowd-powered system for parsing semantic
frames. We then discuss research opportunities in
using this approach to train and improve automated dialog systems in the future.
                                        </td>
                                    </tr>
                                </table>
                            </div>
                        </td>
                    </tr>
                </table>
            </td>
        </tr>
        <tr class="separator">
            <td>
            </td>
        </tr>
        <tr>
            <td width="800px">
                <table border="0" cellspacing="0" cellpadding="0">
                    <tr>
                        <td valign="top" width="10" class="paper">
                            -
                        </td>
                        <td class="paper">
                            Loomis-Thompson, A., <b>Bohus, D.</b> (2013) - <a class="normal" href="http://dl.acm.org/citation.cfm?id=2531751">
                            <i>A Framework for Multimodal Data Collection, Visualization, Annotation and Learning</i></a>, in ICMI'2013,
                            Sydney, Australia [<a class="normal" target="_self" href="javascript:sh('icmi_2013')">abs</a>]
                        </td>
                    </tr>
                    <tr>
                        <td>
                        </td>
                        <td>
                            <div id="icmi_2013" style="display: none">
                                <table class="normal" width="100%">
                                    <tr>
                                        <td valign="top" width="20">
                                            <b>&nbsp;</b>
                                        </td>
                                        <td class="paper_abstract">
                                            The development and iterative refinement of inference models for multimodal systems can be challenging and time intensive. We present a framework for multimodal data collection, visualization, annotation, and learning that enables system developers to build models using various machine learning techniques, and quickly iterate through cycles of development, deployment and refinement.
                                        </td>
                                    </tr>
                                </table>
                            </div>
                        </td>
                    </tr>
                </table>
            </td>
        </tr>
        <tr class="large_separator">
            <td>
            </td>
        </tr>
        <tr class="section_header">
            <td class="section_header" width="800px">
                2012
            </td>
        </tr>
        <tr class="separator">
            <td>
            </td>
        </tr>
        <tr>
            <td width="800px">
                <table border="0" cellspacing="0" cellpadding="0">
                    <tr>
                        <td valign="top" width="10" class="paper">
                            -
                        </td>
                        <td class="paper">
                            Wang, W.Y., <b>Bohus, D.</b>, Kamar, E., Horvitz, E. (2012) - <a class="normal" href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6424200&tag=1"><i>Crowdsourcing the Acquisition
                            of Natural Language Corpora: Methods and Observations</i></a>, in SLT'2012,
                            Miami, USA [<a class="normal" target="_self" href="javascript:sh('slt_2012')">abs</a>]
                        </td>
                    </tr>
                    <tr>
                        <td>
                        </td>
                        <td>
                            <div id="slt_2012" style="display: none">
                                <table class="normal" width="100%">
                                    <tr>
                                        <td valign="top" width="20">
                                            <b>&nbsp;</b>
                                        </td>
                                        <td class="paper_abstract">
                                            We study the opportunity for using crowdsourcing methods to acquire language corpora for use in natural language processing systems. Specifically, we empirically investigate three methods for eliciting natural language sentences that correspond to a given semantic form. The methods convey frame semantics to crowd workers by means of sentences, scenarios, and list-based descriptions. We discuss various performance measures of the crowdsourcing process, and analyze the semantic correctness, naturalness, and biases of the collected language. We highlight research challenges and directions in applying these methods to acquire corpora for natural language processing applications.
                                        </td>
                                    </tr>
                                </table>
                            </div>
                        </td>
                    </tr>
                </table>
            </td>
        </tr>
        <tr class="separator">
            <td>
            </td>
        </tr>
        <tr>
            <td width="800px">
                <table border="0" cellspacing="0" cellpadding="0">
                    <tr>
                        <td valign="top" width="10" class="paper">
                            -
                        </td>
                        <td class="paper">
                            Vinyals, O., <b>Bohus, D.</b>, Caruana, R. (2012) - <a class="normal" href="http://dl.acm.org/citation.cfm?id=2388770"><i>Learning Speaker, Addressee and
                                Overlap Detection Models from Multimodal Streams</i></a>, in ICMI'2012,
                            Santa Monica, USA [<a class="normal" target="_self" href="javascript:sh('learning_speaker')">abs</a>]
                        </td>
                    </tr>
                    <tr>
                        <td>
                        </td>
                        <td>
                            <div id="learning_speaker" style="display: none">
                                <table class="normal" width="100%">
                                    <tr>
                                        <td valign="top" width="20">
                                            <b>&nbsp;</b>
                                        </td>
                                        <td class="paper_abstract">
                                            A key challenge in developing conversational systems is fusing streams of information
                                            provided by different sensors to make inferences about the behaviors and goals of
                                            people. Such systems can leverage visual and audio information collected through
                                            cameras and microphone arrays, including the location of various people, their focus
                                            of attention, body pose, the sound source direction, prosody, and speech recognition
                                            results. In this paper, we explore discriminative learning techniques for making
                                            accurate inferences on the problems of speaker, addressee and overlap detection
                                            in multiparty human-computer dialog. The focus is on finding ways to leverage within-
                                            and across-signal temporal patterns and to construct representations from the raw
                                            streams in an automated manner that are informative for the inference problem. We
                                            present a novel extension to traditional decision trees which allows them to incorporate
                                            and model temporal signals. We contrast these methods with more traditional approaches
                                            where a human expert manually engineers relevant temporal features. The proposed
                                            approach performs well even with relatively small amounts of training data, which
                                            is of practical importance as designing features that are task dependent is time
                                            consuming and not always possible.<br>
                                            <br>
                                        </td>
                                    </tr>
                                </table>
                            </div>
                        </td>
                    </tr>
                </table>
            </td>
        </tr>
        <tr class="separator">
            <td>
            </td>
        </tr>
        <tr>
            <td width="800px">
                <table border="0" cellspacing="0" cellpadding="0" width="100%">
                    <tr>
                        <td valign="top" width="10" class="paper">
                            -
                        </td>
                        <td class="paper">
                            <b>Bohus, D.</b>, Kamar, E., Horvitz, E. (2012) - <a class="normal" href="docs/tsc.pdf">
                                <i>Towards Situated Collaboration</i></a>, in NAACL Workshop on Future Directions
                            and Challenges in Spoken Dialog Systems: Tools and Data [<a class="normal" target="_self"
                                href="javascript:sh('towards_situated_collab')">abs</a>]
                        </td>
                    </tr>
                    <tr>
                        <td>
                        </td>
                        <td>
                            <div id="towards_situated_collab" style="display: none">
                                <table class="normal" width="100%">
                                    <tr>
                                        <td valign="top" width="20">
                                            <b>&nbsp;</b>
                                        </td>
                                        <td class="paper_abstract">
                                            We outline a set of key challenges for dialog management in physically situated
                                            interactive systems, and propose a core shift in perspective that places spoken
                                            dialog in the context of the larger collaborative challenge of managing parallel,
                                            coordinated actions in the open world.<br>
                                            <br>
                                        </td>
                                    </tr>
                                </table>
                            </div>
                        </td>
                    </tr>
                </table>
            </td>
            <td>
            </td>
        </tr>
        <tr class="large_separator">
            <td>
            </td>
        </tr>
        <tr class="section_header">
            <td class="section_header" width="800px">
                2011
            </td>
        </tr>
        <tr class="separator">
            <td>
            </td>
        </tr>
        <tr>
      <td width="800px">     
        <table border=0 cellspacing=0 cellpadding=0 width="100%">
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
<b>Bohus, D.</b>, Horvitz, E. (2011) - <a class="normal" href="docs/icmi2011.pdf"><i>Decisions about Turns in Multiparty Conversation: From Perception to Action</i></a>, in ICMI-2011, Alicante, Spain [<a class="normal" target="_self" href="javascript:sh('turntaking_decisions')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="turntaking_decisions" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
We present a decision-theoretic approach for guiding turn taking in a spoken dialog system operating in multiparty settings. The proposed methodology couples inferences about multiparty conversational dynamics with assessed costs of different outcomes, to guide turn-taking decisions. Beyond considering uncertainties about outcomes arising from evidential reasoning about the state of a conversation, we endow the system with awareness and methods for handling uncertainties stemming from computational delays in its own perception and production. We illustrate via sample cases how the proposed approach makes decisions, and we investigate the behaviors of the proposed methods via a retrospective analysis on logs collected in a multiparty interaction study.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>           
      <td width="800px">     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
<b>Bohus, D.</b>, Horvitz, E. (2011) - <a class="normal" href="docs/sigdial2011.pdf"><i>Multiparty Turn Taking in Situated Dialog: Study, Lessons, and Directions</i></a>, in SIGdial-2011, Portland, OR [<a class="normal" target="_self" href="javascript:sh('turntaking_eval')">abs</a>]&nbsp;[<a class="normal" target="_self" href="javascript:sh('turntaking_eval_supplemental')">Supplemental materials and videos</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="turntaking_eval" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
We report on an empirical study of a multiparty turn taking model for physically situated spoken dialog systems. We discuss subjective and objective performance measures that show how the model, supported with a basic set of sensory competencies and turn-taking policies, can enable interactions with multiple participants in a collaborative task setting. The analysis we conduct brings to the fore several phenomena and frames challenges for managing multiparty turn taking in physically situated interaction.
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="turntaking_eval_supplemental" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
	      	        Videos:
                  <a href="videos/turntakingpapervideos/sample1-full-overhead.wmv">session 1 overhead</a> | 
                  <a href="videos/turntakingpapervideos/sample1-full-scene.wmv">session 1 scene analyis</a> | 
                  <a href="videos/turntakingpapervideos/sample2-full-overhead.wmv">session 2 overhead</a> | 
                  <a href="videos/turntakingpapervideos/sample2-full-scene.wmv">session 2 scene analyis</a>
                  <br>    
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="large_separator">
    <td></td>
    </tr>    

    <tr class="section_header">    
      <td class="section_header">2010</td>
    </tr>     

    <tr class="separator">
    <td></td>
    </tr>    
       
    <tr>
      <td width="800px">     
        <table border=0 cellspacing=0 cellpadding=0 width="100%">
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
<b>Bohus, D.</b>, Horvitz, E. (2010) - <a class="normal" href="docs/dwr_final.pdf"><i>On the Challenges and Opportunities of Physically Situated Dialog</i></a>, in AAAI Fall Symposium on Dialog with Robots, Arlington, VA [<a class="normal" target="_self" href="javascript:sh('dwr')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="dwr" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
We outline several challenges and opportunities for building physically situated systems that can interact in open, dynamic, and relatively unconstrained environments. We review a platform and recent progress on developing computational methods for situated, multiparty, open-world dialog, and highlight the value of representations of the physical surroundings and of harnessing the broader situational context when managing communicative processes such as engagement, turn-taking, language understanding, and dialog management. Finally, we outline an open-world learning challenge that spans these different levels.
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td width="800px">     
        <table border=0 cellspacing=0 cellpadding=0 width="100%">
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
<b>Bohus, D.</b>, Horvitz, E. (2010) - <a class="normal" href="docs/icmi_final2.pdf"><i>Facilitating Multiparty Dialog with Gaze, Gesture and Speech</i></a>, in ICMI'10, Beijing, China [<a class="normal" target="_self" href="javascript:sh('icmi_facilitating')">abs</a>]&nbsp;[<a class="normal" target="_self" href="javascript:sh('icmi_facilitating_supplemental')">Supplemental materials and videos</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="icmi_facilitating" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
We study how synchronized gaze, gesture and speech rendered by an embodied conversational agent can influence the flow of conversations in multiparty settings. We review a computational framework for turn taking that provides the foundation for tracking and communicating intentions to hold, release, or take control of the conversational floor. We then present details of the implementation of the approach in an embodied conversational agent and describe experiments with the system in a shared task setting. Finally, we discuss results showing how the verbal and non-verbal cues used by the avatar can shape the dynamics of multiparty conversation.
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="icmi_facilitating_supplemental" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
	      	        Videos for the interaction segment discussed in <b>Figure 4</b> in the paper:<br> 
                  [<a href="videos/turntakingpapervideos/sample1-segment-overhead.wmv">overhead</a> | 
                  <a href="videos/turntakingpapervideos/sample1-segment-scene.wmv">scene analyis</a> | 
                  <a href="videos/turntakingpapervideos/sample1-full-overhead.wmv">entire session overhead</a> | 
                  <a href="videos/turntakingpapervideos/sample1-full-scene.wmv">entire session scene analyis</a>]
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    


    <tr>
      <td width="800px">     
        <table border=0 cellspacing=0 cellpadding=0 width="100%">
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, Horvitz, E., (2010) - <a class="normal" href="docs/turntaking_msr_tr.pdf"><i>Computational Models for Multiparty Turn-Taking</i></a>, Microsoft Technical Report MSR-TR-2010-115 [<a class="normal" target="_self" href="javascript:sh('turntaking_models')">abs</a>]&nbsp;[<a class="normal" target="_self" href="javascript:sh('turntaking_models_supplemental')">Supplemental materials and videos</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="turntaking_models" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
We describe a computational framework for modeling and managing turn-taking in open-world spoken dialog systems. We present a representation and methodology for tracking the conversational dynamics in multiparty interactions, making floor control decisions, and ren-dering these decisions into appropriate behav-iors. We show how the approach enables an embodied conversational agent to participate in multiparty interactions, and to handle a diversity of natural turn-taking phenomena, including multiparty floor management, barge-ins, restarts, and continuations. Finally, we discuss results and lessons learned from experiments.
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="turntaking_models_supplemental" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
	      	        Videos for the interaction segment discussed in <b>Figure 4</b> in the paper:<br> 
                  [<a href="videos/turntakingpapervideos/sample1-segment-overhead.wmv">overhead</a> | 
                  <a href="videos/turntakingpapervideos/sample1-segment-scene.wmv">scene analyis</a> | 
                  <a href="videos/turntakingpapervideos/sample1-full-overhead.wmv">entire session overhead</a> | 
                  <a href="videos/turntakingpapervideos/sample1-full-scene.wmv">entire session scene analyis</a>]
                  <br><br>    
	      	        Videos for the interaction segment discussed in <b>Figure 6 (Appendix A)</b> in the paper:<br> 
                  [<a href="videos/turntakingpapervideos/sample2-segment-overhead.wmv">overhead</a> | 
                  <a href="videos/turntakingpapervideos/sample2-segment-scene.wmv">scene analyis</a> | 
                  <a href="videos/turntakingpapervideos/sample2-full-overhead.wmv">entire session overhead</a> | 
                  <a href="videos/turntakingpapervideos/sample2-full-scene.wmv">entire session scene analyis</a>]
                  <br>    
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="large_separator">
    <td></td>
    </tr>    

    <tr class="section_header">    
      <td class="section_header" >2009</td>
    </tr>     
    <tr class="separator">
    <td></td>
    </tr>    
       
    <tr>
      <td width="800px">     
        <table border=0 cellspacing=0 cellpadding=0 width="100%">
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
<b>Bohus, D.</b>, Horvitz, E. (2009) - <a class="normal" href="docs/open_world_icmi.pdf"><i>Dialog in the Open World: Platform and Applications</i></a>, in Proceedings of ICMI'09, Boston, MA [<a class="normal" target="_self" href="javascript:sh('open_world_icmi')">abs</a>]&nbsp;[<a class="normal" target="_self" href="videos/ReceptionistICMI.wmv">Receptionist video</a>]&nbsp;[<a class="normal" target="_self" href="videos/multiparty_learning_game.wmv">Questions game video</a>] | <font color="#771111">ICMI'09 outstanding paper award</font>
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="open_world_icmi" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
We review key challenges of developing spoken dialog systems that can engage in interaction with one or multiple participants in open, relatively unconstrained environments. We outline a set of core competencies for open-world dialog, and we describe three prototype systems in this space. The systems harness a common underlying conversational framework which integrates an array of predictive models and component technologies, including speech recognition, head and pose tracking, probabilistic models for scene analysis, multiparty engagement and turn taking, and inferences about user long-term goals and activities. We discuss the current models and showcase their function by means of a sample recorded interaction, and we review results from an observational study of open-world, multiparty dialog in the wild. 
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td width="800px">     
        <table border=0 cellspacing=0 cellpadding=0 width="100%">
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, Horvitz, E. (2009) - <a class="normal" href="docs/detecting_engagement_intentions.pdf"><i>Learning to Predict Engagement with a Spoken Dialog System in Open-World Settings</i></a>, in Proceedings of SIGdial'09, London, UK [<a class="normal" target="_self" href="javascript:sh('detecting_engagement')">abs</a>] [<a class="normal" target="_self" href="javascript:sh('detecting_engagement_note')">note</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="detecting_engagement" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
We describe a machine learning approach that allows an open-world spoken dialog system to learn to predict engagement intentions in situ, from interaction. The proposed approach does not require any developer supervision, and leverages spatiotemporal and attentional features automatically extracted from a visual analysis of people coming into the proximity of the system to produce models that are attuned to the characteristics of the environment the system is placed in. Experimental results indicate that a system using the proposed approach can learn to recognize engagement intentions at low false positive rates (e.g. 2-4%) up to 3-4 seconds prior to the actual moment of engagement.
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="detecting_engagement_note" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
Subsequent experiments with the machine learning infrastructure used in this work have revealed a small defect in the model construction and evaluation. The maximum entropy model was trained in a stepwise fashion, where at each step the next best feature was added to the model; stopping was based on a BIC criterion. During this stepwise model building process, the scoring of features was done by assessing performance on the entire dataset (including train + development folds), instead of exclusively on the train folds. Nevertheless, once a feature to be added to a model was selected, the model was trained exclusively on the training folds, i.e. the corresponding feature weight in the max-ent model was determined based only on the training data, and the evaluation was done on the held-out development fold. Subsequent experiments with a correct setup (where the feature scoring is done only by looking at the training folds) on several problems show that this bug does not significantly affect results. While with a correct setup the numbers reported might differ by small amounts, we believe the general results we have reported in this paper stand. 
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td width="800px">     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper" valign="bottom">
          <b>Bohus, D.</b>, Horvitz, E. (2009) - <a class="normal" href="docs/engagement_model.pdf"><i>Models for Multiparty Engagement in Open-World Dialog</i></a>, in Proceedings of SIGdial'09, London, UK [<a class="normal" target="_self" href="javascript:sh('engagement_model')">abs</a>] | <font color="#771111">SIGdial'09 best paper award</font>
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="engagement_model" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
We present computational models that allow spoken dialog systems to handle multi-participant engagement in open, dynamic environments, where multiple people may enter and leave conversations, and interact with the system and with others in a natural manner. The models for managing the engagement process include components for (1) sensing the en-gagement state, actions and intentions of multiple agents in the scene, (2) making engagement decisions (i.e. whom to engage with, and when) and (3) rendering these decisions in a set of coordinated low-level behaviors in an embodied conversational agent. We review results from a study of interactions "in the wild" with a system that implements such a model.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td width="800px">     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, Horvitz, E. (2009) - <a class="normal" href="docs/openworlddialog.pdf"><i>Open-World Dialog: Challenges, Directions, and Prototype</i></a>, in Proceedings of IJCAI'2009 Workshop on Knowledge and Reasoning in Practical Dialogue Systems, Pasadena, CA [<a class="normal" target="_self" href="javascript:sh('openworlddialog')">abs</a>]&nbsp;[<a class="normal" target="_self" href="videos/SIdemo.wmv">video</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="openworlddialog" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
We present an investigation of open-world dialog, centering on systems that can perform conversational dialog in an open-world context, where multiple people with different needs, goals, and long-term plans may enter, interact, and leave an environment. We outline and discuss a set of challenges and core competencies required for supporting the kind of fluid multiparty interaction that people expect when conversing and collaborating with other people. Then, we focus as a concrete example on the challenges faced by receptionists who field requests at the entries to corporate buildings. We review the subtleties and difficulties of creating an automated receptionist that can work with people on solving their needs with the ease and etiquette expected from a human receptionist. Finally, we review details of the construction and operation of a working prototype.
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td width="800px">     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          Li, X., Nguyen, P., Zweig, G., <b>Bohus, D.</b> (2009) - <a class="normal" href="docs/icassp09.pdf"><i>Leveraging Multiple Query Logs to Improve Language Models for Spoken Query Recognition</i></a>, in Proceedings of ICASSP'09, Taipei, Taiwan [<a class="normal" target="_self" href="javascript:sh('query_logs')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="query_logs" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
A voice search system requires a speech interface that can correctly
recognize spoken queries uttered by users. The recognition performance
strongly relies on a robust language model. In this work, we
present the use of multiple data sources, with the focus on query logs,
in improving ASR language models for a voice search application.
Our contributions are three folds: (1) the use of text queries from
web search and mobile search in language modeling; (2) the use of
web click data to predict query forms from business listing forms;
and (3) the use of voice query logs in creating a positive feedback
loop. Experiments show that by leveraging these resources, we can
achieve recognition performance comparable to, or even better than,
that of a previously deploy system where a large amount of spoken
query transcripts are used in language modeling.
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="large_separator">
    <td></td>
    </tr>    

    <tr class="section_header">    
      <td class="section_header" width="800px">2008</td>
    </tr>     
    <tr class="separator">
    <td></td>
    </tr>    
       
    <tr>
      <td width="800px">     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, Zweig, G., Nguyen, P., Li, X. (2008) - <a class="normal" href="docs/slt_paper_submission.pdf"><i>Joint N-Best Rescoring for Repeated Utterances in Spoken Dialog Systems</i></a>, in Proceedings of SLT'08, Goa, India [<a class="normal" target="_self" href="javascript:sh('repeats2')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="repeats2" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
Due to speech recognition errors, repetitions are a frequent phenomenon in spoken dialog systems. In previous work we have proposed a joint decoding model that can leverage structural relationships between repeated utterances for improving recogni-tion performance. In this paper we extend this work in two directions. First, we propose a direct, classification-based model for the same task. The new model can leverage features that were fundamentally hard to capture in the previous framework (e.g. spellings, false-starts, etc.) and leads to an additional performance improvement. Second, we show how both models can be used to perform a combined rescoring of two n-best lists that are part of a repetition pair.
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td width="800px">     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          Zweig, G., <b>Bohus, D.</b>, Li, X., Nguyen, P. (2008) - <a class="normal" href="docs/repeats.pdf"><i>Structured Models for Joint Decoding of Repeated Utterances</i></a>, in Proceedings of InterSpeech'08, Brisbane, Australia [<a class="normal" target="_self" href="javascript:sh('repeats')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="repeats" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
Due to speech recognition errors, repetition can be a frequent occurrence in voice-search applications. While a proper treatment of this phenomenon requires the joint modeling of two or more utterances simultaneously, currently deployed systems typically treat the utterances independently. In this paper, we analyze the structure of repetitions and find that in at least one commercial directory assistance application, repetitions follow simple structural transformations more than 70% of the time. We present preliminary results that suggest that significant gains are possible by explicitly modeling this structure in a joint decoding process.
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td width="800px">     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, Li, X., Nguyen, P., and Zweig, G. (2008) - <a class="normal" href="docs/sigdial_final_paper.pdf"><i>Learning N-Best Correction Models from Implicit User Feedback in a Multi-modal Local Search Application</i></a>, in Proceedings of SIGdial'08, Columbus, OH [<a class="normal" target="_self" href="javascript:sh('nbest_correction')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="nbest_correction" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
We describe a novel n-best correction model that can leverage implicit user feedback (in the form of clicks) to improve performance in a multi-modal speech-search application. The proposed model works in two stages. First, the n-best list generated by the speech recognizer is expanded with additional candidates, based on confusability information captured via user click statistics. In the second stage, this expanded list is rescored and pruned to produce a more accurate and compact n-best list. Re-sults indicate that the proposed n-best correction model leads to significant improvements over the existing baseline, as well as other traditional n-best rescoring approaches.
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, Rudnicky, A. (2008) - <a class="normal" href="http://dx.doi.org/10.1016/j.csl.2008.10.001"><i>The RavenClaw dialog management framework: architecture and systems</i></a>, in Computer Speech and Language, DOI:10.1016/j.csl.2008.10.001 [<a class="normal" target="_self" href="javascript:sh('ravenclaw_journal')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="ravenclaw_journal" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
In this paper, we describe RavenClaw, a plan-based, task-independent dialog management framework. RavenClaw isolates the domain-specific aspects of the dialog control logic from domain-independent conversational skills, and in the process facilitates rapid development of mixed-initiative systems operating in complex, task-oriented domains. System developers can focus exclusively on describing the dialog task control logic, while a large number of domain-independent conversational skills such as error handling, timing and turn-taking are transparently supported and enforced by the RavenClaw dialog engine. To date, RavenClaw has been used to construct and deploy a large number of systems, spanning different domains and interaction styles, such as information access, guidance through procedures, command-and-control, medical diagnosis, etc. The framework has easily adapted to all of these domains, indicating a high degree of versatility and scalability.
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="large_separator">
    <td></td>
    </tr>    


    <tr class="section_header">    
      <td class="section_header" width="800px">2007</td>
    </tr>     
    <tr class="separator">
    <td></td>
    </tr>    
    
    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b> (2007) - <a class="normal" href="docs/Thesis.pdf"><i>Error Awareness and Recovery in Conversational Spoken Language Interfaces</i></a>, Ph.D. Dissertation, CS-07-124, Carnegie Mellon University, Pittsburgh, PA [<a class="normal" target="_self" href="javascript:sh('thesis')">abs</a>] [<a class="normal" target="_self" href="javascript:sh('thesis_note')">note</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="thesis" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
One of the most important and persistent problems in the development of conversational spoken language interfaces is their lack of robustness when confronted with understanding-errors. Most of these errors stem from limitations in current speech recognition technology, and, as a result, appear across all domains and interaction types. There are two approaches towards increased robustness: prevent the errors from happening, or recover from them through conversation, by interacting with the users. <br>
In this dissertation we have engaged in a research program centered on the second approach. We argue that three capabilities are needed in order to seamlessly and efficiently recover from errors: (1) systems must be able to detect the errors, preferably as soon as they happen, (2) systems must be equipped with a rich repertoire of error recovery strategies that can be used to set the conversation back on track, and (3) systems must know how to choose optimally between different recovery strategies at run-time, i.e. they must have good error recovery policies. This work makes a number of contributions in each of these areas. <br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="thesis_note" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
Subsequent experiments with the machine learning infrastructure that has been used in parts of this work have revealed a small defect in the procedure for logistic regression model construction and evaluation. In places where such models were constructed in a stepwise model building process (chapters 6 and 7), the scoring of features was done by assessing performance on the entire dataset (including train + development folds), instead of exclusively on the train folds. Nevertheless, once a feature to be added to a model was selected, the model was trained exclusively on the training folds, i.e. the corresponding feature weight in the max-ent model was determined based only on the training data, and the evaluation was done on the held-out development fold. Subsequent experiments with a correct setup (where the feature scoring is done only by looking at the training folds) on several problems show that this bug does not significantly affect results. While with a correct setup the numbers reported in cross-validation might differ by small amounts, we believe the results reported in this work stand. 
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, and Rudnicky, A. (2007) - <a class="normal" href="docs/islearning-final.pdf"><i>Implicitly-supervised learning in spoken language interfaces: an application to the confidence annotation problem</i></a>, in Proceedings of SIGdial 2007, Antwerp, Belgium [<a class="normal" target="_self" href="javascript:sh('islearning')">abs</a>] [<a class="normal" target="_self" href="javascript:sh('islearning_note')">note</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="islearning" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
In this paper we propose the use of a novel learning paradigm in spoken language in-terfaces – implicitly-supervised learning. The central idea is to extract a supervision signal online, directly from the user, from certain patterns that occur naturally in the conversation. The approach eliminates the need for developer supervision and facili-tates online learning and adaptation. As a first step towards better understanding its properties, advantages and limitations, we have applied the proposed approach to the problem of confidence annotation. Experi-mental results indicate that we can attain performance similar to that of a fully su-pervised model, without any manual labe-ling. In effect, the system learns from its own experiences with the users.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="islearning_note" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
Subsequent experiments with the machine learning infrastructure used in this work have revealed a small defect in the model construction and evaluation. During the stepwise model building process, the scoring of features was done by assessing performance on the entire dataset (including train + development folds), instead of exclusively on the train folds. Nevertheless, once a feature to be added to a model was selected, the model was trained exclusively on the training folds, i.e. the corresponding feature weight in the max-ent model was determined based only on the training data, and the evaluation was done on the held-out development fold. Subsequent experiments with a correct setup (where the feature scoring is done only by looking at the training folds) on several problems show that this bug does not significantly affect results. While with a correct setup the numbers reported might differ by small amounts, we believe the general results we have reported in this paper stand. 
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          Ai, H., Raux, A., <b>Bohus, D.</b>, Eskenazi, M., and Litman, D. (2007) - <a class="normal" href="docs/ai07_final.pdf"><i>Comparing Spoken Dialog Corpora Collected with Recruited Subjects versus Real Users</i></a>, in Proceedings of SIGdial 2007, Antwerp, Belgium [<a class="normal" target="_self" href="javascript:sh('comparing_corpora')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="comparing_corpora" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
Empirical spoken dialog research often involves
the collection and analysis of a dialog
corpus. However, it is not well understood
whether and how a corpus of dialogs collected
using recruited subjects differs from
a corpus of dialogs obtained from real users.
In this paper we use Let’s Go Lab, a platform
for experimenting with a deployed spoken
dialog bus information system, to address
this question. Our first corpus is collected
by recruiting subjects to call Let’s Go
in a standard laboratory setting, while our
second corpus consists of calls from real
users calling Let’s Go during its operating
hours. We quantitatively characterize the
two collected corpora using previously proposed
measures from the spoken dialog literature,
then discuss the statistically significant
similarities and differences between the
two corpora with respect to these measures.
For example, we find that recruited subjects
talk more and speak faster, while real users
ask for more help and more frequently interrupt
the system. In contrast, we find no
difference with respect to dialog structure.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, Raux, A., Harris, T., Eskenazi, M., and Rudnicky, A. (2007) - <a class="normal" href="docs/olympus-final.pdf"><i>Olympus: an open-source framework for conversational spoken language interface research</i></a>, in HLT-NAACL 2007 workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technology, Rochester, NY [<a class="normal" target="_self" href="javascript:sh('olympus')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="olympus" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
We introduce Olympus, a freely available framework for research in conversational interfaces. Olympus’ open, transparent, flexible, modular and scalable nature facilitates the development of large-scale, real-world systems, and enables research leading to technological and scientific advances in conversational spoken language interfaces. In this paper, we describe the overall architecture, several systems spanning different domains, and a number of current research efforts supported by Olympus.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, Grau, S., Huggins-Daines, D., Keri, V., Krishna, G., Kumar, R., Raux, A., and Tomko, S. (2007) - <a class="normal" href="docs/conquest-paper-final.pdf"><i>Conquest - an Open-Source Dialog System for Conferences</i></a>, in Proceedings of HLT-NAACL 2007, Rochester, NY [<a class="normal" target="_self" href="javascript:sh('conquest')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="conquest" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
We describe ConQuest, an open-source, reusable spoken dialog system that provides technical program information dur-ing conferences. The system uses a transparent, modular and open infrastructure, and aims to enable applied research in spoken language interfaces. The conference domain is a good platform for applied research since it permits periodical redeployments and evaluations with a real user-base. In this paper, we describe the system’s functionality, overall architecture, and we discuss two initial deployments.
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          Tetreault, J., and <b>Bohus, D.</b>, (2007) - <a class="normal" href="docs/naacl07-confidence.pdf"><i>Estimating the Reliability of MDP Policies: a Confidence Interval Approach</i></a>, in HLT-NAACL 2007, Rochester, NY [<a class="normal" target="_self" href="javascript:sh('mdp_confidence')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="mdp_confidence" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
Data sparsity is one of the major issues that NLP researchers always wrestle with. That is, does one have enough data to make reliable conclusions in an experiment? Using Reinforcement Learning to improve a spoken dialogue system is
no exception. Past approaches in this area have simply assumed that there was enough collected data to derive reliable dialog control policies or used thousands of user simulations to overcome the sparsity issue. In this paper we present a methodology for numerically constructing confidence bounds on the expected reward for a constructed policy, and use these bounds to better estimate the reliability of that policy. We apply this methodology to a prior
experiment of using MDP's to predict the best features to include in a model of the dialogue state. Our results show that policies developed in the prior work were not as reliable as previously determined but the overall ranking of features remains the same.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="large_separator">
    <td></td>
    </tr>    

    <tr class="section_header">    
      <td class="section_header" width="800px">2006</td>
    </tr>     
    <tr class="separator">
    <td></td>
    </tr>    
    
    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, Langner, B., Raux, A., Black, A., Eskenazi, M. and Rudnicky A. (2006) - <a class="normal" href="docs/SLTPaper.pdf"><i>Online Supervised Learning of Non-understanding Recovery Policies</i></a>, in SLT-2006, Palm Beach, Aruba [<a class="normal" target="_self" href="javascript:sh('online_nonu')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="online_nonu" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
Spoken dialog systems typically use a limited number of nonunderstanding
recovery strategies and simple heuristic policies to
engage them (e.g. first ask user to repeat, then give help, then
transfer to an operator). We propose a supervised, online method
for learning a non-understanding recovery policy over a large set
of recovery strategies. The approach consists of two steps: first, we
construct runtime estimates for the likelihood of success of each
recovery strategy, and then we use these estimates to construct a
policy. An experiment with a publicly available spoken dialog
system shows that the learned policy produced a 12.5% relative
improvement in the non-understanding recovery rate.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, and Rudnicky, A. (2006) - <a class="normal" href="docs/khypotheses.pdf"><i>A K Hypotheses + Other Belief Updating Model</i></a>, in AAAI Workshop on Statistical and Empirical Approaches to Spoken Dialogue Systems, 2006, Boston, MA [<a class="normal" target="_self" href="javascript:sh('khypotheses')">abs</a>] [<a class="normal" target="_self" href="javascript:sh('khypotheses_note')">note</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="khypotheses" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
Spoken dialog systems typically rely on recognition confidence
scores to guard against potential misunderstandings.
While confidence scores can provide an initial assessment
for the reliability of the information obtained from the user,
ideally systems should leverage information that is available
in subsequent user responses to update and improve the accuracy
of their beliefs. We present a machine-learning
based solution for this problem. We use a compressed representation
of beliefs that tracks up to k hypotheses for each
concept at any given time. We train a generalized linear
model to perform the updates. Experimental results show
that the proposed approach significantly outperforms heuristic
rules used for this task in current systems. Furthermore, a
user study with a mixed-initiative spoken dialog system
shows that the approach leads to significant gains in task
success and in the efficiency of the interaction, across a
wide range of recognition error-rates.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="khypotheses_note" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
Subsequent experiments with the machine learning infrastructure used in this work have revealed a small defect in the model construction and evaluation. During the stepwise model building process, the scoring of features was done by assessing performance on the entire dataset (including train + development folds), instead of exclusively on the train folds. Nevertheless, once a feature to be added to a model was selected, the model was trained exclusively on the training folds, i.e. the corresponding feature weight in the max-ent model was determined based only on the training data, and the evaluation was done on the held-out development fold. Subsequent experiments with a correct setup (where the feature scoring is done only by looking at the training folds) on several problems show that this bug does not significantly affect results. While with a correct setup the numbers reported in cross-validation might differ by small amounts, we believe the general results we have reported in this paper stand. 
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          Raux, A., <b>Bohus, D.</b>, Langner, B., Black, A., and Eskenazi, M. (2006) - <a class="normal" href="docs/letsgo_interspeech2006.pdf"><i>Doing Research in a Deployed Spoken Dialog System: One Year of Let's Go! Public Experience</i></a>, in Interspeech-2006, Pittsburgh, PA [<a class="normal" target="_self" href="javascript:sh('letsgo2')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="letsgo2" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
This paper describes our work with Let’s Go, a telephone-based
bus schedule information system that has been in use by
the Pittsburgh population since March 2005. Results from
several studies show that while task success correlates
strongly with speech recognition accuracy, other aspects of
dialogue such as turn-taking, the set of error recovery strategies,
and the initiative style also significantly impact system
performance and user behavior.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="large_separator">
    <td></td>
    </tr>    

    <tr class="section_header">    
      <td class="section_header" width="800px">2005</td>
    </tr>     
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, and Rudnicky, A. (2005) - <a class="normal" href="docs/bu.pdf">
          <i>Constructing Accurate Beliefs in Spoken Dialog Systems</i></a>, in ASRU-2005, San Juan, Puerto Rico [<a class="normal" target="_self" href="javascript:sh('beliefs1')">abs</a>] [<a class="normal" href="docs/BeliefUpdatingPoster.ppt">poster</a>] [<a class="normal" target="_self" href="javascript:sh('beliefs1_note')">note</a>] 
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="beliefs1" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
We propose a novel approach for constructing more accurate
beliefs over concept values in spoken dialog systems by
integrating information across multiple turns in the conversation.
In particular, we focus our attention on updating the confidence
score of the top hypothesis for a concept, in light of subsequent
user responses to system confirmation actions. Our data-driven
approach bridges previous work in confidence annotation and
correction detection, providing a unified framework for belief
updating. The approach significantly outperforms heuristic rules
currently used in most spoken dialog systems.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="beliefs1_note" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
Subsequent experiments with the machine learning infrastructure used in this work have revealed a small defect in the model construction and evaluation. During the stepwise model building process, the scoring of features was done by assessing performance on the entire dataset (including train + development folds), instead of exclusively on the train folds. Nevertheless, once a feature to be added to a model was selected, the model was trained exclusively on the training folds, i.e. the corresponding feature weight in the max-ent model was determined based only on the training data, and the evaluation was done on the held-out development fold. Subsequent experiments with a correct setup (where the feature scoring is done only by looking at the training folds) on several problems show that this bug does not significantly affect results. While with a correct setup the numbers reported in cross-validation might differ by small amounts, we believe the general results we have reported in this paper stand. 
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, and Rudnicky, A. (2005) - <a class="normal" href="docs/errorh.pdf"><i>Error Handling in the RavenClaw dialog management architecture</i></a>, in HLT-EMNLP-2005, Vancouver, CA [<a class="normal" target="_self" href="javascript:sh('error_architecture')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="error_architecture" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
We describe the error handling architecture
underlying the RavenClaw dialog
management framework. The architecture
provides a robust basis for current and future
research in error detection and recovery.
Several objectives were pursued in its
development: task-independence, ease-ofuse,
adaptability and scalability. We describe
the key aspects of architectural design
which confer these properties, and
discuss the deployment of this architecture
in a number of spoken dialog systems
spanning several domains and interaction
types. Finally, we outline current research
projects supported by this architecture.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, and Rudnicky, A. (2005) - <a class="normal" href="docs/nonu_final.pdf"><i>Sorry, I Didn't Catch That! - An Investigation of Non-understanding Errors and Recovery Strategies</i></a>, in SIGdial-2005, Lisbon, Portugal [<a class="normal" target="_self" href="javascript:sh('nonu_strategies')">abs</a>] [<a class="normal" href="docs/sidct_book.pdf">sigdial book chapter</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="nonu_strategies" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
We present results from an extensive empirical analysis of non-understanding 
errors and ten non-understanding recovery strategies, based on a corpus of 
dialogs collected with a spoken dialog system that handles conference room 
reservations. More specifically, the issues we investigate are: what are the 
main sources of non-understanding errors? What is the impact of these errors on 
global performance? How do various strategies for recovery from non-
understandings compare to each other? What are the relationships between these 
strategies and subsequent user response types, and which response types are more
likely to lead to successful recovery? Can dialog performance be improved by 
using a smarter policy for engaging the non-understanding recovery strategies? 
If so, can we learn such a policy from data? Whenever available, we compare and 
contrast our results with other studies in the literature. Finally, we summarize 
the lessons learned and present our plans for future work inspired by this 
analysis.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, and Rudnicky, A. (2005) - <a class="normal" href="docs/dbohus_interspeech05.pdf"><i>A Principled Approach for Rejection Threshold Optimization in Spoken Dialog Systems</i></a>, in Interspeech-2005, Lisbon, Portugal [<a class="normal" target="_self" href="javascript:sh('rej_threshold')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="rej_threshold" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
A common design pattern in spoken dialog systems is to reject
an input when the recognition confidence score falls below a
preset rejection threshold. However, this introduces a
potentially non-optimal tradeoff between various types of
errors such as misunderstandings and false rejections. In this
paper, we propose a data-driven method for determining the
relative costs of these errors, and then use these costs to
optimize state-specific rejection thresholds. We illustrate the
use of this approach with data from a spoken dialog system
that handles conference room reservations. The results
obtained confirm our intuitions about the costs of the errors,
and are consistent with anecdotal evidence gathered throughout
the use of the system.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          Raux, A., Langner, B., <b>Bohus, D.</b>, Black, A., and Eskenazi, M. (2005) - <a class="normal" href="docs/letsgo.pdf"><i>Let's Go Public! Taking a Spoken Dialog System to the Real World</i></a>, in Interspeech-2005, Lisbon, Portugal [<a class="normal" target="_self" href="javascript:sh('letsgo')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="letsgo" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
In this paper, we describe how a research spoken dialog system
was made available to the general public. The Let’s Go Public
spoken dialog system provides bus schedule information to the
Pittsburgh population during off-peak times. This paper describes
the changes necessary to make the system usable for the general
public and presents analysis of the calls and strategies we have
used to ensure high performance.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="large_separator">
    <td></td>
    </tr>    

    <tr class="section_header">    
      <td class="section_header" width="800px">2004</td>
    </tr>     
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b> (2004) - <a class="normal" href="docs/proposal.pdf"><i>Error Awareness and Recovery in Task-Oriented Spoken Dialogue Systems</i></a>, Ph.D Thesis Proposal, Carnegie Mellon University, Pittsburgh, PA [<a class="normal" target="_self" href="javascript:sh('proposal')">abs</a>] [<a class="normal" href="docs/proposal.ppt">slides</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="proposal" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">	      	        
A persistent and important problem in spoken language interfaces is their lack of robustness when faced with understanding errors. The problem is present across all domains and interaction types, and stems primarily from the unreliability of the speech recognition process. I propose to alleviate this problem by (1) endowing spoken dialogue systems with better error awareness, (2) constructing a richer repertoire of error recovery strategies, and (3) developing a practical data-driven approach for making error handling decisions. The proposed work will address questions and make contributions in each of these three areas. For the first part, I propose to develop a belief updating mechanism that integrates confidence annotation and correction detection into a unified framework, and allows spoken dialogue systems to continuously track the reliability of the information they use. For the second part, I propose to implement and investigate an extended set of error recovery strategies addressing common problems in human-computer dialogue. Finally, I plan to bring these two capabilities together in a scalable reinforcement-learning based approach for making error handling decisions in task-oriented spoken dialogue systems. <br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, and Rudnicky, A. (2004) - <a class="normal" href="docs/strategies_full.pdf"><i>Task-Independent Conversational Strategies in the RavenClaw Dialogue Management Framework</i></a>, unpublished manuscript [<a class="normal" target="_self" href="javascript:sh('tics')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="tics" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">	      	        
We present the implementation of task-independ¬ent conversational strategies in the RavenClaw dialogue management framework. The proposed approach decouples the implementation and the control of these strategies from the actual system task, and brings forth several advantages: it in-creases the consistency in the interaction style, while at the same time it lessens the development and testing efforts by allowing for the easy reuse of these strategies across different systems. We plan to illustrate the repertoire of task-independent con-versational strategies in the RavenClaw dialogue management framework by giving a live demon-stration of RoomLine, a spoken dialogue system for conference room reservation and scheduling.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          Aist, G., <b>Bohus, D.</b>, Boven, B., Campana, E., Early, S., Phan, S. (2004) - <i>Initial Development of a Voice-Activated Astronaut Assistant for Procedural Tasks: From Need to Concept to Prototype</i>, in Journal of Interactive Instruction Development, Volume 16, Nr. 3, Winter 2004, pp 32-36
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="large_separator">
    <td></td>
    </tr>    

    <tr class="section_header">    
      <td class="section_header" width="800px">2003</td>
    </tr>     
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, and Rudnicky A. (2003) - <a class="normal" href="docs/ravenclaw.ps"><i>RavenClaw: Dialog Management Using Hierarchical Task Decomposition and an Expectation Agenda</i></a>, in Eurospeech-2003, Geneva, Switzerland [<a class="normal" target="_self" href="javascript:sh('ravenclaw')">abs</a>] [<a class="normal" href="docs/RavenClawPoster.ppt">poster</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="ravenclaw" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
We describe RavenClaw, a new dialog management framework developed as a successor to the Agenda architecture used in the CMU Communicator. RavenClaw introduces a clear separation between task and discourse behavior specification, and allows rapid development of dialog management components for spoken dialog systems operating in complex, goal-oriented domains. The system development effort is focused entirely on the specification of the dialog task, while a rich set of domain-independent conversational behaviors are transparently generated by the dialog engine.  To date, RavenClaw has been applied to five different domains allowing us to draw some preliminary conclusions as to the generality of the approach. We briefly describe our experience in developing these systems.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          Aist, G., Dowding, J., Hockey, B.A., Rayner, M., Hieronymus, J., <b>Bohus, D.</b>, Boven, B., Blaylock, N., Campana, E., Early, S., Gorrell, G., and Phan, S. (2003) - <a class="normal" href="docs/talkingthroughprocedures.pdf"><i>Talking through procedures: An intelligent Space Station procedure assistant</i></a>, in Demo Session at EACL-2003, Budapest, Hungary [<a class="normal" target="_self" href="javascript:sh('talkingthroughprocedures')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="talkingthroughprocedures" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
We present a prototype system aimed at
providing spoken dialogue support for
complex procedures aboard the International
Space Station. The system allows
navigation one line at a time or in larger
steps. Other user functions include issuing
spoken corrections, requesting images
and diagrams, recording voice notes and
spoken alarms, and controlling audio volume.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="large_separator">
    <td></td>
    </tr>    

    <tr class="section_header">    
      <td class="section_header" width="800px">2002</td>
    </tr>     
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, and Rudnicky A. (2002) - <a class="normal" href="docs/larri.ps"><i>LARRI: A Language-Based Maintenance and Repair Assistant</i></a>, in IDS-2002, Kloster Irsee, Germany [<a class="normal" target="_self" href="javascript:sh('larri')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="larri" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
LARRI (Language-based Agent for Retrieval of Repair Information) is a dialog-based system for support of maintenance and repair domains, characterized by large amounts of documentation and by procedural information. LARRI is based on an architecture developed by Carnegie Mellon University for the DARPA Communicator program and is integrated with a wearable computer system developed by the Wearable Computing group at Carnegie Mellon University. 
LARRI adapts a dialog-management architecture developed and optimized for a telephone-based problem solving task (travel planning), and applies it to a very different domain -- aircraft maintenance. The system was taken on a field trial on two occasions where it was used by professional aircraft mechanics. We found that our architecture, AGENDA, extended readily to a multi-modal and multi-media framework. At the same time we found that assumptions that were reasonable in a services domain turn out to be inappropriate for a maintenance domain. Apart from the need to manage integration between input modes and output modalities, we found that the system needed to support multiple categories of tasks and that a different balance between user and system goals was required. A significant problem in the maintenance domain is the need to assimilate and make available for language processing appropriate domain information. <br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, and Rudnicky A. (2002) - <a class="normal" href="docs/tr-190.pdf"><i>Integrating Multiple Knowledge Sources for Utterance-Level Confidence Annotation in the CMU Communicator Spoken Dialog System</i></a>, Technical Report CS-190, Carnegie Mellon University, Pittsburgh, PA [<a class="normal" target="_self" href="javascript:sh('imksulcaccsds')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="imksulcaccsds" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
In the recent years, automated speech recognition has been the main drive behind 
the advent of spoken language interfaces, but at the same time a severe limiting 
factor in the development of these systems. We believe that increased robustness 
in the face of recognition errors can be achieved by making the systems aware of 
their own misunderstandings, and employing appropriate recovery techniques when 
breakdowns in interaction occur. In this paper we address the first problem: the 
development of an utterance-level confidence annotator for a spoken dialog 
system. After a brief introduction to the CMU Communicator spoken dialog system 
(which provided the target platform for the developed annotator), we cast the 
confidence annotation problem as a machine learning classification task, and 
focus on selecting relevant features and on empirically identifying the best 
classification techniques for this task. The results indicate that significant 
reductions in classification error rate can be obtained using several different 
classifiers. Furthermore, we propose a data driven approach to assessing the 
impact of the errors committed by the confidence annotator on dialog 
performance, with a view to optimally fine-tuning the annotator. Several models 
were constructed, and the resulting error costs were in accordance with our 
intuition. We found, surprisingly, that, at least for a mixed-initiative spoken 
dialog system as the CMU Communicator, these errors trade-off equally over a 
wide operating characteristic range.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="large_separator">
    <td></td>
    </tr>    

    <tr class="section_header">    
      <td class="section_header" width="800px">2001</td>
    </tr>     
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, and Rudnicky, A. (2001) - <a class="normal" href="docs/mcmu.ps"><i>Modeling the Cost of Misunderstandings in the CMU Communicator Dialog System</i></a>, in ASRU-2001, Madonna di Campiglio, Italy [<a class="normal" target="_self" href="javascript:sh('mcmccds')">abs</a>] [<a class="normal" href="docs/misunderstandings.ppt">slides</a>] [<a class="normal" href="docs/ASRU01Poster.ppt">poster</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="mcmccds" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
We describe a data-driven approach that allows us to quantify the costs of various types of errors made by the utterance-level confidence annotator in the Carnegie Mellon Communicator system. Knowing these costs we can determine the optimal tradeoff point between these errors, and tune the confidence annotator accordingly. We describe several models, based on concept transmission efficiency. The models fit our data quite well and the relative costs of errors are in accordance with our intuition. We also find, surprisingly, that for a mixed-initiative system such as the CMU Communicator, false positive and false negative errors trade-off equally over a wide operating range.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          Carpenter P., Jin C., Wilson D., Zhang R., <b>Bohus, D.</b>, and Rudnicky A. (2001) - <a class="normal" href="docs/confidence.ps"><i>Is This Conversation on Track?</i></a>, in Eurospeech-2001, Aalborg, Denmark [<a class="normal" target="_self" href="javascript:sh('itcot')">abs</a>] [<a class="normal" href="docs/confidence.ppt">slides</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="itcot" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
Confidence annotation allows a spoken dialog system to accurately assess the likelihood of misunderstanding at the utterance level and to avoid breakdowns in interaction. We describe experiments that assess the utility of features from the decoder, parser and dialog levels of processing. We also investigate the effectiveness of various classifiers, including Bayesian Networks, Neural Networks, SVMs, Decision Trees, AdaBoost and Naive Bayes, to combine this information into an utterance-level confidence metric. We found that a combination of a subset of the features considered produced promising results with several of the classification algorithms considered, e.g., our Bayesian Network classifier produced a 45.7% relative reduction in confidence assessment error and a 29.6% reduction relative to a handcrafted rule.
<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="large_separator">
    <td></td>
    </tr>    

    <tr class="section_header">    
      <td class="section_header" width="800px">2000</td>
    </tr>     
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b> (2000) - <a class="normal" href="docs/stospun.ps"><i>Stochastic Speech Understanding for Human-Computer Dialogue</i></a>, Romanian Journal of Information Science and Technology, Volume 4, No 3-4/2001, p.261 [<a class="normal" target="_self" href="javascript:sh('ssuhcd')">abs</a>] [<a class="normal" href="docs/sbsasds_ro.ps.gz">Romanian version</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="ssuhcd" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
The thesis describes a domain and language independent semantic analyzer for spoken dialog systems built using a stochastic approach based on Hidden Markov Models and the case grammar formalism.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    

    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b> (2000) - <a class="normal" href="docs/grad_project.ps.gz"><i>Stochastically-Based Semantic Analysis in Human-Computer Dialog</i></a>, Graduate Thesis, Politechnica University of Timisoara, Romania
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="separator">
    <td></td>
    </tr>    
    
    <tr>
      <td>     
        <table border=0 cellspacing=0 cellpadding=0>
        <tr>
          <td valign="top" width="10" class="paper">-</td>
          <td class="paper">
          <b>Bohus, D.</b>, and Boldea, M. (2000) - <a class="normal" href="docs/wbtcds_lrec2000.ps.gz"><i>A Web-based Text Corpora Development System</i></a>, in LREC-2000, Athens, Greece [<a class="normal" target="_self" href="javascript:sh('awbtcds')">abs</a>]
          </td>
        </tr>
        <tr>
          <td></td>
        	<td>
	      	  <div id="awbtcds" style="display:none">
	      	    <table class="normal" width="100%">
	      	      <tr>
	      	        <td valign="top" width="20"><b>&nbsp;</b></td>
	      	        <td class="paper_abstract">
One of the most important starting points for any NLP endeavor is the construction of text corpora of appropriate size and quality. This paper presents a web-based text corpora development system that focuses both on the size and the quality of these corpora. The quantitative problem is solved by using the Internet as a practically limitless resource of texts. To ensure a certain quality, we enrich the text with relevant information to be fit for further use by resolving in an integrated manner the problems of diacritic characters restoration, lexical ambiguity resolution and morphosyntactic annotation. Although at this moment it is targeted at texts in Romanian, a number of mechanisms have been provided that allows it to be easily adapted to other languages.<br><br>
                  </td>
                </tr>
          		</table>
            </div>
          </td>
        </tr>
        </table>
      </td>
    </tr>
    <tr class="large_separator">
    <td></td>
    </tr>    
    <tr class="large_separator">
    <td></td>
    </tr>    
    <tr class="large_separator">
    <td></td>
    </tr>    
    </table>
</div>
</div>
</HTML>
